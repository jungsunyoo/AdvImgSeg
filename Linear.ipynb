{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojungsun0/AdvImgSeg/blob/master/Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTqWQ1GTSqeY",
        "outputId": "28314f39-8eaa-4516-a1ee-1abfa060c240"
      },
      "source": [
        "!pip install thop"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.6/dist-packages (0.0.31.post2005241907)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from thop) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->thop) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2jZOoMMQ2NA",
        "outputId": "5ef07af7-c58f-4fa0-9791-8726d09525a8"
      },
      "source": [
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from thop import profile, clever_format\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "import sys\n",
        "%cd /content/gdrive/MyDrive/'Colab Notebooks'/SimCLR\n",
        "\n",
        "\n",
        "import utils\n",
        "from model_separation import Model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Colab Notebooks/SimCLR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXpV3lvBQ75r"
      },
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self, num_class, pretrained_path):\n",
        "#         super(Net, self).__init__()\n",
        "\n",
        "#         # encoder\n",
        "#         self.f = Model().f\n",
        "#         # classifier\n",
        "#         self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "#         self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.f(x)\n",
        "#         feature = torch.flatten(x, start_dim=1)\n",
        "#         out = self.fc(feature)\n",
        "#         return out\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_class, pretrained_path):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.f = Model().f\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
        "        self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        x1 = x[:,:,0,:]\n",
        "        x2 = x[:,:,1,:]\n",
        "\n",
        "        feature1 = torch.flatten(x1, start_dim=1)\n",
        "        feature2 = torch.flatten(x2, start_dim=1)\n",
        "        # out1 = self.g1(feature1)\n",
        "        # out2 = self.g2(feature2)\n",
        "        # feature = torch.flatten(x, start_dim=1)\n",
        "        out = self.fc(feature1)\n",
        "        return out\n",
        "\n",
        "# train or test for one epoch\n",
        "def train_val(net, data_loader, train_optimizer):\n",
        "    is_train = train_optimizer is not None\n",
        "    net.train() if is_train else net.eval()\n",
        "\n",
        "    total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)\n",
        "    with (torch.enable_grad() if is_train else torch.no_grad()):\n",
        "        for data, target in data_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            out = net(data)\n",
        "            loss = loss_criterion(out, target)\n",
        "\n",
        "            if is_train:\n",
        "                train_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                train_optimizer.step()\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            prediction = torch.argsort(out, dim=-1, descending=True)\n",
        "            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "            total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
        "\n",
        "            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'\n",
        "                                     .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,\n",
        "                                             total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))\n",
        "\n",
        "    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2WPPuCZQ8EC",
        "outputId": "6353576d-7bd2-440a-b159-a771f4bf0ca5"
      },
      "source": [
        "# import os\n",
        "# rootdir = '../'\n",
        "model_path = 'results/Model3_twovec/128_0.5_200_256_100_model.pth'\n",
        "# model_path = os.path.join(rootdir, path)\n",
        "#'results/128_0.5_200_256_100_model.pth'\n",
        "batch_size = 256 #512\n",
        "epochs = 50 #100\n",
        "\n",
        "train_data = CIFAR10(root='data', train=True, transform=utils.train_transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
        "test_data = CIFAR10(root='data', train=False, transform=utils.test_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
        "\n",
        "model = Net(num_class=len(train_data.classes), pretrained_path=model_path).cuda()\n",
        "for param in model.f.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "flops, params = profile(model, inputs=(torch.randn(1, 3, 32, 32).cuda(),))\n",
        "flops, params = clever_format([flops, params])\n",
        "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],\n",
        "            'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc@1'].append(train_acc_1)\n",
        "    results['train_acc@5'].append(train_acc_5)\n",
        "    test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    results['test_acc@5'].append(test_acc_5)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
        "    data_frame.to_csv('results/Model3_twovec/linear_statistics.csv', index_label='epoch')\n",
        "    if test_acc_1 > best_acc:\n",
        "        best_acc = test_acc_1\n",
        "        torch.save(model.state_dict(), 'results/Model3_twovec/linear_model.pth')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/196 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
            "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
            "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "\u001b[91m[WARN] Cannot find rule for <class 'torchvision.models.resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "\u001b[91m[WARN] Cannot find rule for <class '__main__.Net'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
            "# Model Params: 23.52M FLOPs: 1.30G\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: [1/50] Loss: 0.8557 ACC@1: 70.83% ACC@5: 97.05%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [1/50] Loss: 0.6072 ACC@1: 78.63% ACC@5: 98.97%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [2/50] Loss: 0.7648 ACC@1: 73.11% ACC@5: 97.80%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [2/50] Loss: 0.5848 ACC@1: 79.44% ACC@5: 99.00%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [3/50] Loss: 0.7498 ACC@1: 73.82% ACC@5: 97.80%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [3/50] Loss: 0.5517 ACC@1: 81.06% ACC@5: 99.11%: 100%|██████████| 40/40 [00:05<00:00,  7.59it/s]\n",
            "Train Epoch: [4/50] Loss: 0.7473 ACC@1: 73.89% ACC@5: 97.82%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [4/50] Loss: 0.5676 ACC@1: 80.09% ACC@5: 99.23%: 100%|██████████| 40/40 [00:05<00:00,  7.47it/s]\n",
            "Train Epoch: [5/50] Loss: 0.7332 ACC@1: 74.28% ACC@5: 97.87%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [5/50] Loss: 0.5430 ACC@1: 80.99% ACC@5: 99.19%: 100%|██████████| 40/40 [00:05<00:00,  7.36it/s]\n",
            "Train Epoch: [6/50] Loss: 0.7262 ACC@1: 74.57% ACC@5: 97.98%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [6/50] Loss: 0.5362 ACC@1: 81.13% ACC@5: 99.30%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [7/50] Loss: 0.7273 ACC@1: 74.61% ACC@5: 97.86%: 100%|██████████| 196/196 [00:51<00:00,  3.77it/s]\n",
            "Test Epoch: [7/50] Loss: 0.5378 ACC@1: 81.31% ACC@5: 99.23%: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s]\n",
            "Train Epoch: [8/50] Loss: 0.7149 ACC@1: 74.96% ACC@5: 98.00%: 100%|██████████| 196/196 [00:52<00:00,  3.71it/s]\n",
            "Test Epoch: [8/50] Loss: 0.5378 ACC@1: 81.36% ACC@5: 99.20%: 100%|██████████| 40/40 [00:05<00:00,  7.35it/s]\n",
            "Train Epoch: [9/50] Loss: 0.7179 ACC@1: 75.01% ACC@5: 97.92%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [9/50] Loss: 0.5218 ACC@1: 81.81% ACC@5: 99.25%: 100%|██████████| 40/40 [00:05<00:00,  7.31it/s]\n",
            "Train Epoch: [10/50] Loss: 0.7195 ACC@1: 74.71% ACC@5: 97.89%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [10/50] Loss: 0.5456 ACC@1: 80.96% ACC@5: 99.16%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [11/50] Loss: 0.7126 ACC@1: 75.04% ACC@5: 97.95%: 100%|██████████| 196/196 [00:52<00:00,  3.77it/s]\n",
            "Test Epoch: [11/50] Loss: 0.5406 ACC@1: 80.95% ACC@5: 99.17%: 100%|██████████| 40/40 [00:05<00:00,  7.40it/s]\n",
            "Train Epoch: [12/50] Loss: 0.7095 ACC@1: 75.27% ACC@5: 97.98%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [12/50] Loss: 0.5337 ACC@1: 81.39% ACC@5: 99.12%: 100%|██████████| 40/40 [00:05<00:00,  7.40it/s]\n",
            "Train Epoch: [13/50] Loss: 0.7030 ACC@1: 75.38% ACC@5: 97.93%: 100%|██████████| 196/196 [00:51<00:00,  3.82it/s]\n",
            "Test Epoch: [13/50] Loss: 0.5198 ACC@1: 81.80% ACC@5: 99.25%: 100%|██████████| 40/40 [00:05<00:00,  7.56it/s]\n",
            "Train Epoch: [14/50] Loss: 0.7056 ACC@1: 75.32% ACC@5: 97.99%: 100%|██████████| 196/196 [00:51<00:00,  3.84it/s]\n",
            "Test Epoch: [14/50] Loss: 0.5189 ACC@1: 81.94% ACC@5: 99.27%: 100%|██████████| 40/40 [00:06<00:00,  6.31it/s]\n",
            "Train Epoch: [15/50] Loss: 0.7032 ACC@1: 75.33% ACC@5: 97.98%: 100%|██████████| 196/196 [00:49<00:00,  3.93it/s]\n",
            "Test Epoch: [15/50] Loss: 0.5125 ACC@1: 81.86% ACC@5: 99.32%: 100%|██████████| 40/40 [00:05<00:00,  7.54it/s]\n",
            "Train Epoch: [16/50] Loss: 0.6918 ACC@1: 75.57% ACC@5: 98.17%: 100%|██████████| 196/196 [00:49<00:00,  3.97it/s]\n",
            "Test Epoch: [16/50] Loss: 0.5263 ACC@1: 81.68% ACC@5: 99.20%: 100%|██████████| 40/40 [00:05<00:00,  7.58it/s]\n",
            "Train Epoch: [17/50] Loss: 0.6983 ACC@1: 75.57% ACC@5: 98.12%: 100%|██████████| 196/196 [00:49<00:00,  3.97it/s]\n",
            "Test Epoch: [17/50] Loss: 0.5268 ACC@1: 81.58% ACC@5: 99.29%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [18/50] Loss: 0.6938 ACC@1: 75.83% ACC@5: 98.07%: 100%|██████████| 196/196 [00:49<00:00,  3.98it/s]\n",
            "Test Epoch: [18/50] Loss: 0.5273 ACC@1: 81.40% ACC@5: 99.18%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [19/50] Loss: 0.6898 ACC@1: 75.88% ACC@5: 97.98%: 100%|██████████| 196/196 [00:49<00:00,  3.93it/s]\n",
            "Test Epoch: [19/50] Loss: 0.5094 ACC@1: 82.11% ACC@5: 99.32%: 100%|██████████| 40/40 [00:05<00:00,  7.48it/s]\n",
            "Train Epoch: [20/50] Loss: 0.6998 ACC@1: 75.42% ACC@5: 97.98%: 100%|██████████| 196/196 [00:49<00:00,  3.93it/s]\n",
            "Test Epoch: [20/50] Loss: 0.5094 ACC@1: 81.97% ACC@5: 99.36%: 100%|██████████| 40/40 [00:05<00:00,  7.59it/s]\n",
            "Train Epoch: [21/50] Loss: 0.6911 ACC@1: 75.77% ACC@5: 98.04%: 100%|██████████| 196/196 [00:50<00:00,  3.87it/s]\n",
            "Test Epoch: [21/50] Loss: 0.5115 ACC@1: 81.81% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.41it/s]\n",
            "Train Epoch: [22/50] Loss: 0.6927 ACC@1: 75.66% ACC@5: 98.08%: 100%|██████████| 196/196 [00:49<00:00,  3.98it/s]\n",
            "Test Epoch: [22/50] Loss: 0.5221 ACC@1: 81.92% ACC@5: 99.20%: 100%|██████████| 40/40 [00:05<00:00,  7.41it/s]\n",
            "Train Epoch: [23/50] Loss: 0.6899 ACC@1: 75.70% ACC@5: 97.98%: 100%|██████████| 196/196 [00:48<00:00,  4.01it/s]\n",
            "Test Epoch: [23/50] Loss: 0.5057 ACC@1: 82.31% ACC@5: 99.28%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [24/50] Loss: 0.6894 ACC@1: 75.72% ACC@5: 97.97%: 100%|██████████| 196/196 [00:49<00:00,  3.96it/s]\n",
            "Test Epoch: [24/50] Loss: 0.5084 ACC@1: 82.15% ACC@5: 99.23%: 100%|██████████| 40/40 [00:05<00:00,  7.46it/s]\n",
            "Train Epoch: [25/50] Loss: 0.6880 ACC@1: 76.09% ACC@5: 98.04%: 100%|██████████| 196/196 [00:49<00:00,  3.98it/s]\n",
            "Test Epoch: [25/50] Loss: 0.4997 ACC@1: 82.69% ACC@5: 99.26%: 100%|██████████| 40/40 [00:05<00:00,  7.51it/s]\n",
            "Train Epoch: [26/50] Loss: 0.6816 ACC@1: 76.07% ACC@5: 98.11%: 100%|██████████| 196/196 [00:49<00:00,  3.93it/s]\n",
            "Test Epoch: [26/50] Loss: 0.5059 ACC@1: 82.03% ACC@5: 99.31%: 100%|██████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Train Epoch: [27/50] Loss: 0.6863 ACC@1: 76.05% ACC@5: 98.00%: 100%|██████████| 196/196 [00:48<00:00,  4.02it/s]\n",
            "Test Epoch: [27/50] Loss: 0.4959 ACC@1: 82.65% ACC@5: 99.29%: 100%|██████████| 40/40 [00:05<00:00,  7.56it/s]\n",
            "Train Epoch: [28/50] Loss: 0.6867 ACC@1: 75.97% ACC@5: 98.11%: 100%|██████████| 196/196 [00:48<00:00,  4.03it/s]\n",
            "Test Epoch: [28/50] Loss: 0.4994 ACC@1: 82.43% ACC@5: 99.35%: 100%|██████████| 40/40 [00:05<00:00,  7.60it/s]\n",
            "Train Epoch: [29/50] Loss: 0.6853 ACC@1: 76.04% ACC@5: 98.11%: 100%|██████████| 196/196 [00:48<00:00,  4.05it/s]\n",
            "Test Epoch: [29/50] Loss: 0.5022 ACC@1: 82.49% ACC@5: 99.30%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [30/50] Loss: 0.6830 ACC@1: 75.92% ACC@5: 98.09%: 100%|██████████| 196/196 [00:49<00:00,  3.99it/s]\n",
            "Test Epoch: [30/50] Loss: 0.4945 ACC@1: 82.78% ACC@5: 99.35%: 100%|██████████| 40/40 [00:05<00:00,  7.64it/s]\n",
            "Train Epoch: [31/50] Loss: 0.6820 ACC@1: 76.07% ACC@5: 98.04%: 100%|██████████| 196/196 [00:49<00:00,  4.00it/s]\n",
            "Test Epoch: [31/50] Loss: 0.5051 ACC@1: 82.25% ACC@5: 99.28%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [32/50] Loss: 0.6836 ACC@1: 76.04% ACC@5: 98.04%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [32/50] Loss: 0.4943 ACC@1: 82.56% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.50it/s]\n",
            "Train Epoch: [33/50] Loss: 0.6779 ACC@1: 76.41% ACC@5: 98.08%: 100%|██████████| 196/196 [00:48<00:00,  4.04it/s]\n",
            "Test Epoch: [33/50] Loss: 0.4974 ACC@1: 82.39% ACC@5: 99.29%: 100%|██████████| 40/40 [00:05<00:00,  7.57it/s]\n",
            "Train Epoch: [34/50] Loss: 0.6774 ACC@1: 76.35% ACC@5: 98.16%: 100%|██████████| 196/196 [00:48<00:00,  4.06it/s]\n",
            "Test Epoch: [34/50] Loss: 0.4944 ACC@1: 82.65% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.60it/s]\n",
            "Train Epoch: [35/50] Loss: 0.6831 ACC@1: 76.31% ACC@5: 97.97%: 100%|██████████| 196/196 [00:47<00:00,  4.09it/s]\n",
            "Test Epoch: [35/50] Loss: 0.4980 ACC@1: 82.54% ACC@5: 99.44%: 100%|██████████| 40/40 [00:05<00:00,  7.68it/s]\n",
            "Train Epoch: [36/50] Loss: 0.6825 ACC@1: 76.03% ACC@5: 98.10%: 100%|██████████| 196/196 [00:48<00:00,  4.08it/s]\n",
            "Test Epoch: [36/50] Loss: 0.4983 ACC@1: 82.66% ACC@5: 99.24%: 100%|██████████| 40/40 [00:05<00:00,  7.69it/s]\n",
            "Train Epoch: [37/50] Loss: 0.6765 ACC@1: 76.29% ACC@5: 98.10%: 100%|██████████| 196/196 [00:48<00:00,  4.08it/s]\n",
            "Test Epoch: [37/50] Loss: 0.4963 ACC@1: 82.64% ACC@5: 99.24%: 100%|██████████| 40/40 [00:05<00:00,  7.52it/s]\n",
            "Train Epoch: [38/50] Loss: 0.6818 ACC@1: 75.94% ACC@5: 98.07%: 100%|██████████| 196/196 [00:51<00:00,  3.80it/s]\n",
            "Test Epoch: [38/50] Loss: 0.4965 ACC@1: 82.63% ACC@5: 99.33%: 100%|██████████| 40/40 [00:05<00:00,  7.28it/s]\n",
            "Train Epoch: [39/50] Loss: 0.6758 ACC@1: 76.33% ACC@5: 98.07%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [39/50] Loss: 0.5031 ACC@1: 82.41% ACC@5: 99.33%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [40/50] Loss: 0.6764 ACC@1: 76.31% ACC@5: 98.07%: 100%|██████████| 196/196 [00:50<00:00,  3.90it/s]\n",
            "Test Epoch: [40/50] Loss: 0.4919 ACC@1: 82.53% ACC@5: 99.35%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n",
            "Train Epoch: [41/50] Loss: 0.6806 ACC@1: 76.24% ACC@5: 97.97%: 100%|██████████| 196/196 [00:49<00:00,  3.93it/s]\n",
            "Test Epoch: [41/50] Loss: 0.4911 ACC@1: 82.74% ACC@5: 99.44%: 100%|██████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Train Epoch: [42/50] Loss: 0.6775 ACC@1: 76.20% ACC@5: 98.11%: 100%|██████████| 196/196 [00:49<00:00,  3.98it/s]\n",
            "Test Epoch: [42/50] Loss: 0.4879 ACC@1: 82.84% ACC@5: 99.40%: 100%|██████████| 40/40 [00:05<00:00,  7.49it/s]\n",
            "Train Epoch: [43/50] Loss: 0.6739 ACC@1: 76.21% ACC@5: 98.04%: 100%|██████████| 196/196 [00:49<00:00,  3.94it/s]\n",
            "Test Epoch: [43/50] Loss: 0.4923 ACC@1: 82.46% ACC@5: 99.44%: 100%|██████████| 40/40 [00:05<00:00,  7.63it/s]\n",
            "Train Epoch: [44/50] Loss: 0.6763 ACC@1: 76.42% ACC@5: 97.99%: 100%|██████████| 196/196 [00:49<00:00,  3.96it/s]\n",
            "Test Epoch: [44/50] Loss: 0.5138 ACC@1: 81.74% ACC@5: 99.37%: 100%|██████████| 40/40 [00:05<00:00,  7.44it/s]\n",
            "Train Epoch: [45/50] Loss: 0.6704 ACC@1: 76.45% ACC@5: 98.22%: 100%|██████████| 196/196 [00:50<00:00,  3.92it/s]\n",
            "Test Epoch: [45/50] Loss: 0.5033 ACC@1: 82.21% ACC@5: 99.40%: 100%|██████████| 40/40 [00:05<00:00,  7.50it/s]\n",
            "Train Epoch: [46/50] Loss: 0.6727 ACC@1: 76.46% ACC@5: 98.17%: 100%|██████████| 196/196 [00:49<00:00,  3.94it/s]\n",
            "Test Epoch: [46/50] Loss: 0.5029 ACC@1: 82.05% ACC@5: 99.39%: 100%|██████████| 40/40 [00:05<00:00,  7.37it/s]\n",
            "Train Epoch: [47/50] Loss: 0.6748 ACC@1: 76.49% ACC@5: 98.13%: 100%|██████████| 196/196 [00:50<00:00,  3.88it/s]\n",
            "Test Epoch: [47/50] Loss: 0.4999 ACC@1: 82.32% ACC@5: 99.38%: 100%|██████████| 40/40 [00:05<00:00,  7.47it/s]\n",
            "Train Epoch: [48/50] Loss: 0.6726 ACC@1: 76.57% ACC@5: 98.06%: 100%|██████████| 196/196 [00:50<00:00,  3.86it/s]\n",
            "Test Epoch: [48/50] Loss: 0.5129 ACC@1: 81.83% ACC@5: 99.45%: 100%|██████████| 40/40 [00:05<00:00,  7.43it/s]\n",
            "Train Epoch: [49/50] Loss: 0.6761 ACC@1: 76.21% ACC@5: 98.07%: 100%|██████████| 196/196 [00:51<00:00,  3.81it/s]\n",
            "Test Epoch: [49/50] Loss: 0.4956 ACC@1: 82.66% ACC@5: 99.25%: 100%|██████████| 40/40 [00:05<00:00,  7.42it/s]\n",
            "Train Epoch: [50/50] Loss: 0.6759 ACC@1: 76.36% ACC@5: 98.05%: 100%|██████████| 196/196 [00:50<00:00,  3.91it/s]\n",
            "Test Epoch: [50/50] Loss: 0.4942 ACC@1: 82.35% ACC@5: 99.38%: 100%|██████████| 40/40 [00:05<00:00,  7.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}